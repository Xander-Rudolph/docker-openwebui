services:
  stable-diffusion-webui-nvid:
    profiles:
      - NVIDIA
    # If you want to run a build instead of pulling the image:
    # build:
    #   context: ./stable-diffusion
    #   dockerfile: Dockerfile
    #   args:
    #     addextensions: "true"  # Control whether extensions are cloned
    #     addmodels: "true"       # Control whether models are downloaded
    # OR after you have built
    # image: docker-openwebui-stable-diffusion-webui-nvid:latest 
    # OR so you don't have to build
    image: ghcr.io/xander-rudolph/stable-diffusion:latest
    container_name: stable-diffusion-webui
    ports:
      - "${SD_WEBUI_PORT}:7860"
    volumes:
      - sd_models:/data/models
      - sd_output:/data/output
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  
  ollama-nvid:
    profiles:
      - NVIDIA
    image: ollama/ollama
    container_name: ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "${OLLAMA_PORT}:11434"
    restart: always
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  ollama-rocm:
    profiles:
      - AMD
    image: ollama/ollama:rocm
    container_name: ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "${OLLAMA_PORT}:11434"
    restart: always    
    devices:
      - /dev/kfd
      - /dev/dri

  open-webui:
    profiles:
      - AMD
      - NVIDIA
      - default
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "${OPEN_WEBUI_PORT}:8080"
    environment:
      - AUTOMATIC1111_BASE_URL=http://stable-diffusion-webui:${SD_WEBUI_PORT}/
      - ENABLE_IMAGE_GENERATION=True
      - OLLAMA_BASE_URL=http://${OLLAMA_HOST:-ollama}:${OLLAMA_PORT}
      - ENABLE_RAG_WEB_SEARCH=${ENABLE_RAG_WEB_SEARCH:-True}
      - RAG_WEB_SEARCH_ENGINE=${RAG_WEB_SEARCH_ENGINE:-searxng}
      - RAG_WEB_SEARCH_RESULT_COUNT=${RAG_WEB_SEARCH_RESULT_COUNT:-3}
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=${RAG_WEB_SEARCH_CONCURRENT_REQUESTS:-30}
      - SEARXNG_QUERY_URL=http://${SEARXNG_HOSTNAME:-searxng}:${SEARXNG_PORT:-8080}/search?q=<query>&format=json
    volumes:
      - owui_data:/app/backend/data
    restart: always

  searxng:
    profiles:
      - AMD
      - NVIDIA
      - default
    container_name: searxng
    image: searxng/searxng:latest
    ports:
      # NOTE: this should really be 8888 https://github.com/searxng/searxng/issues/4274
      - "${SEARXNG_PORT:-8080}:8080"  
    volumes:
      - search:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
      - TRUSTED_NETWORKS="10.0.0.0/8,192.168.0.0/16,127.0.0.1/8,172.16.0.0/14" # allow calls from most A and C networks as well as docker networks (172.16.0.0/14 â†’ Covers 172.16.0.0 - 172.19.255.255)
      - SEARXNG_DEBUG=true
      - SEARXNG_API__ENABLED=true
      - SEARXNG_API__REQUIRE_API_KEY=false
      # this doesn't work but you need to set   
      # formats:
      # - html
      # - json <--
      #  Otherwise the requests will get rejected for queries to the pod... raised and rejected
      # https://github.com/searxng/searxng/issues/4273
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    depends_on:
      - open-webui

volumes:
  search:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${SEARCH_DIR}"
  sd_models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${SD_MODELS_DIR}"
  sd_output:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${SD_OUTPUT_DIR}"
  ollama_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${LAMA_DATA_DIR}"
  owui_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${OWUI_DATA_DIR}"
