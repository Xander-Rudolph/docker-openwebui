services:
  stable-diffusion-webui-nvid:
    profiles:
      - NVIDIA
    # If you want to run a build instead of pulling the image:
    build:
      context: ./stable-diffusion
      dockerfile: Dockerfile
      args:
        addextensions: "true"  # Control whether extensions are cloned
        addmodels: "true"       # Control whether models are downloaded
    # OR after you have built
    # image: docker-openwebui-stable-diffusion-webui-nvid:latest 
    # OR so you don't have to build
    # image: ghcr.io/xander-rudolph/stable-diffusion:latest
    # OR if you want it loaded up with tools
    # image: ghcr.io/xander-rudolph/stable-diffusion:ext-mdl
    container_name: ${SD_HOST:-stable-diffusion}
    ports:
      - "${SD_WEBUI_PORT}:7860"
    volumes:
      - sd_models:/data/models
      - sd_output:/data/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  
  ollama-nvid:
    profiles:
      - NVIDIA
    image: ollama/ollama
    container_name: ${OLLAMA_HOST:-ollama}
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "${OLLAMA_PORT}:11434"
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  ollama-rocm:
    profiles:
      - AMD
    image: ollama/ollama:rocm
    container_name: ${OLLAMA_HOST:-ollama}
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "${OLLAMA_PORT}:11434"
    devices:
      - /dev/kfd
      - /dev/dri

  open-webui:
    profiles:
      - AMD
      - NVIDIA
      - default
    image: ghcr.io/open-webui/open-webui:main
    # TODO: there has to be a way to do this so I can swap the default model and load it...
    # command: sh -c 'curl -X POST http://${OLLAMA_HOST:-ollama}:${OLLAMA_PORT:-11434}/api/pull -d "{\"name\": \"deepseek-r1\"}"'
    container_name: ${OWUI_HOST:-open-webui}
    ports:
      - "${OPEN_WEBUI_PORT}:8080"
    environment:
      - AUTOMATIC1111_BASE_URL=http://${SD_HOST:-stable-diffusion}:${SD_WEBUI_PORT}/
      - ENABLE_IMAGE_GENERATION=True
      - IMAGE_GENERATION_ENGINE=automatic1111
      - AUTOMATIC1111_API_AUTH=
      - AUTOMATIC1111_CFG_SCALE=7.0
      - AUTOMATIC1111_SAMPLER=DPM++ 2M
      - AUTOMATIC1111_SCHEDULER=Automatic
      - IMAGE_GENERATION_MODEL="v1-5-pruned-emaonly.safetensors"
      - AUDIO_TTS_ENGINE=openai
      - AUDIO_TTS_MODEL=tts-1-hd
      - AUDIO_TTS_VOICE=alloy
      - AUDIO_TTS_SPLIT_ON=punctuation
      - AUDIO_TTS_OPENAI_API_BASE_URL=http://${TTS_HOST:-tts}:${TTS_PORT}/v1
      - AUDIO_TTS_OPENAI_API_KEY="${TTS_PORT}:${TTS_HOST:-tts}"
      - OLLAMA_BASE_URL=http://${OLLAMA_HOST:-ollama}:${OLLAMA_PORT}
      - ENABLE_RAG_WEB_SEARCH=${ENABLE_RAG_WEB_SEARCH:-True}
      - RAG_WEB_SEARCH_ENGINE=${RAG_WEB_SEARCH_ENGINE:-searxng}
      - RAG_WEB_SEARCH_RESULT_COUNT=${RAG_WEB_SEARCH_RESULT_COUNT:-3}
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=${RAG_WEB_SEARCH_CONCURRENT_REQUESTS:-30}
      - SEARXNG_QUERY_URL=http://${SEARXNG_HOSTNAME:-searxng}:${SEARXNG_PORT:-8080}/search?q=<query>&format=json
    volumes:
      - owui_data:/app/backend/data

  searxng:
    profiles:
      - AMD
      - NVIDIA
      - default
    container_name: ${SEARXNG_HOST:-searxng}
    # If you want to run a build instead of pulling the image:
    build:
      context: ./searxng
      dockerfile: Dockerfile
    # Base image with unmodified settings.xml which fails due to lack of accept json
    #  Otherwise the requests will get rejected for queries to the pod... raised and rejected: https://github.com/searxng/searxng/issues/4273
    # formats:
    # - html
    # - json <--
    # image: searxng/searxng:latest
    # OR #TODO: get this to actually work...
    # image: ghcr.io/xander-rudolph/searxng:latest
    # additionally the wrong port is mapped by default so make sure you use 8080
    # https://github.com/searxng/searxng/issues/4274
    ports:
      - "${SEARXNG_PORT:-8080}:8080"  
    volumes:
      - search:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
      - TRUSTED_NETWORKS="10.0.0.0/8,192.168.0.0/16,127.0.0.1/8,172.16.0.0/14" # allow calls from most A and C networks as well as docker networks (172.16.0.0/14 â†’ Covers 172.16.0.0 - 172.19.255.255)
      - SEARXNG_DEBUG=true
      - SEARXNG_API__ENABLED=true
      - SEARXNG_API__REQUIRE_API_KEY=false
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    depends_on:
      - open-webui

  tts-nvid:
    container_name: ${TTS_HOST:-tts}
    profiles:
      - NVIDIA
    image: ghcr.io/matatonic/openedai-speech
    environment:
      API_KEY: "${TTS_PORT}:${TTS_HOST:-tts}"
    ports:
      - "${TTS_PORT}:8000"
    volumes:
      - voices:/app/voices
      - tts_config:/app/config
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  tts-rocm:
    profiles:
      - AMD
    container_name: ${TTS_HOST:-tts}
    image: ghcr.io/matatonic/openedai-speech:rocm
    ports:
      - "${TTS_PORT}:8000"
    volumes:
      - voices:/app/voices
      - tts_config:/app/config
    devices:
      - /dev/kfd
      - /dev/dri

volumes:
  search:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${SEARCH_DIR}"
  sd_models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${SD_MODELS_DIR}"
  sd_output:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${SD_OUTPUT_DIR}"
  ollama_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${LAMA_DATA_DIR}"
  owui_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${OWUI_DATA_DIR}"
  voices:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${VOICES_DIR}"
  tts_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${TTS_CONFIG_DIR}"
